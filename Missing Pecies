1.  In the event log table for LDP* pipelines, the data quality results are logged under events of type 'flow_progress' and stored inside the details column in a nested JSON structure:



details:flow_progress: contains information about a pipeline’s execution progress

details:flow_progress.data_quality: contains the data quality results (expectations, dropped_records, etc.)

details:flow_progress:data_quality.expectations: specifically holds the expectation results

* Databricks has recenlty open-sourced this solution, integrating it into the Apache Spark ecosystem under the name Spark Declarative Pipelines (SDP).

2.  The “validate” option in the notebook identifies syntax or configuration errors in the pipeline definition before execution, reducing the risk of runtime failures or writing partial data.

3.  dlt.expect_all enforces all the specified data quality rules, writes both valid and invalid records to the target table, and captures metrics about any rule violations.



dlt.expect would not fully meet the requirements because it applies expectations individually but doesn’t automatically enforce all of them together as a group. 
Similarly, dlt.expect_or_drop removes individual invalid records, and dlt.expect_or_fail stops the pipeline on individual rule violations.
You can group multiple expectations together and specify collective actions using the functions dlt.expect_all_or_drop, and dlt.expect_all_or_fail.



Note: Databricks has recenlty open-sourced this solution, integrating it into the Apache Spark ecosystem under the name Spark Declarative Pipelines (SDP).

4.  DataFrameWriter.mode defines the writing behaviour when data or table already exists.

Options include:

append: Append contents of the DataFrame to existing data.

overwrite: Overwrite existing data.

error or errorifexists: Throw an exception if data already exists.

ignore: Silently ignore this operation if data already exists.



This errorifexists or error is the default save mode. If the table already exists, it will throw the error message Error: pyspark.sql.utils.AnalysisException: table already exists.



The "employees_performance" table has a date column. So, in order to be able to compare employees' performance across time, each new batch of data with new date should be appended into the table using the append mode.

5.  Reading table’s changes, captured by CDF, using spark.read means that you are reading them as a static source. So, each time you run the query, all table’s changes (starting from the specified startingVersion) will be read.



The query in the question then writes the data in mode “overwrite” to the target table, which completely overwrites the table at each execution.

6.  The Workspace Jobs UI does not allow data engineers to programmatically trigger a multi-task job run. It’s a graphical interface that requires manual interaction and cannot be used for automated or code-based job execution.



While the REST API, Command-line interface (CLI), and Databricks SDKs all provide programmatic ways to run jobs.

7.  Python wheel is a binary distribution format for installing custom Python code packages on Databricks Clusters.



A wheel is a ZIP-format archive with the .whl extension.

8.  The next step the team should take is to create a foreign catalog in Unity Catalog using the existing mysql_connection, as Lakehouse Federation in Databricks allows Unity Catalog to register external data sources like MySQL as foreign catalogs. 
This makes the tables accessible and queryable in a governed, secure way without moving the data, which aligns with the team’s goal.

9.  Databricks Asset Bundles are a feature of the Databricks CLI. To enable the CLI to authenticate to Databricks without managing Databricks secrets,
  it’s recommended to use OAuth token federation for a Databricks service principal in the target workspace.

10.  The correct command is databricks bundle generate, because it allows the data engineer to generate bundle configuration for a resource that already exists in your Databricks workspace. 
    This process generates a YAML definition of a job, pipeline, app, or dashboard and automatically downloads any artifacts it references, such as notebooks.
11.  Databricks-to-Databricks Delta Sharing enables secure, open, and real-time sharing of tables, notebooks, volumes, and ML models with other Databricks clients. 
    This does not require them to have access to the same Databricks account or workspace. With Unity Catalog, the company can ensure fine-grained access control and governance. 
    This approach is efficient, scalable, and adheres to enterprise-grade security standards.

12.  Data engineers must understand how materializing results is different between views and tables on Databricks, and how to reduce total compute and storage cost associated with each materialization depending on the scenario.



    Consider using a view when:
    
    Your query is not complex. Because views are computed on demand, the view is re-computed every time the view is queried. 
    So, frequently querying complex queries with joins and subqueries increases compute costs
    
    You want to reduce storage costs. Views do not require additional storage resources.
    
    Consider using a gold table when:
    
    Multiple downstream queries consume the table, so you want to avoid re-computing complex ad-hoc queries every time.
    
    Query results should be computed incrementally from a data source that is continuously or incrementally growing.

13.   The correct Databricks CLI command that allows a data engineer to list all runs of a job that started at or after a specific time is databricks jobs list-runs --job-id <job-id> --start-time-from <time-value>, 
      as --start-time-from is the proper parameter used to filter job runs based on their start time in the Databricks CLI.
14.  To enable Automatic Liquid Clustering on a Delta table in Databricks, two prerequisites are required:



    Table must be a Unity Catalog-managed table
    
    Automatic Liquid Clustering works only on tables managed by Unity Catalog. External tables are currently not supported.
    
    Table must have predictive optimization enabled
    
    Predictive optimization provides the system with insights on access patterns, which Liquid Clustering leverages to automatically optimize data layout.
